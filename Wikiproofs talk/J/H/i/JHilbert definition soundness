== Thoughts on safe definitions ==

What is the difference between an unsound definition like constx (def ((constx) x) from [[User:GrafZahl/unsound-def.gh]]) and a sound one like ⊤ or ⊥ in [[Principia Mathematica propositional logic]], or ≤ in [[Basic arithmetic]]? It is that the sound ones do not depend on the choice of the dummy variable.  For example, for ⊥, p ∧ ¬ p is equivalent to q ∧ ¬ q.  This is not just a metalogical argument; it is a theorem we can prove within the system (and indeed the relevant proofs are Tautology and Contradiction in [[Principia Mathematica propositional logic]] and LessEqual in [[Basic arithmetic]]).  No such proof is forthcoming with constx.  In other words, noone can expect to get safe definitions (which include dummy variables) for free; the safety of a definition must be proved.

(The preceding paragraph probably states things which were already understood by Raph and the rest of you already, but perhaps could be put in a FAQ or something, as pointing to LessEqual is, at least for me, more self-explanatory than relying exclusively on abstract terms like "alpha conversion").

How does this help us design a definition mechanism? Well, it makes me lean towards "One traditional approach would be to use fresh variables" from [http://sites.google.com/a/ghilbert.org/ghilbert/documents/definitions] (or at least, what I think that paragraph is saying).  I'm not sure I see the problem with the lack of a unique expansion if the variables are truly fresh. That paragraph concludes with "Of course, in standard the expanded theorem would still be provable through an alpha conversion, but that violates the spirit and the letter of the goal of definitions being conservative".  The theorem LessEqual which I opened with is such an alpha conversion.  I don't understand the second half of the sentence, however (what theorem is provable with the definition and not without it in this scheme?).

Now, I could see the appeal if building in bound variables, as described in [http://sites.google.com/a/ghilbert.org/ghilbert/documents/definitions], reduces the need to tediously prove alpha conversions (and/or has other benefits like nuking cv/value conversions). But if you want a gut reaction, I feel like I ''should'' need to prove the alpha conversions and that for the proof verifier to know enough to make alpha conversions (or something equivalent) seems like more magic than I'm used to from a metamath-style system. [[User:Kingdon|Kingdon]] 15:15, 27 June 2010 (UTC)

== Automatic abbreviations and non-automatic definitions ==

Here I make the case for abbreviations, which have no dummy variables and are applied automatically as much as practical, and definitions, which have dummy variables and are applied automatically much more sparingly if at all. My example for the former is ∉ ([[Interface:General set theory]]) and ≠ ([[Interface:First-order logic]]). Metamath has pages and pages of theorems about ∉ and ≠ ([http://us.metamath.org/mpeuni/mmtheorems17.html#mm1659s Negated equality and membership], theorems wne and following), and since they express no new mathematical truths, it seems nice (and feasible) to dispense with them. My example for wanting definitions to be non-automatic is <code>(∅)</code> or <code>(singleton s)</code> at [[General set theory]]. A theorem like <code>EmptySet</code> or <code>Singleton</code> is, once proved, usable without reference to its dummy variables.  In fact, once exported, it can be used this way.  However, when used within [[General set theory]], the definition is expanded to include a dummy variable which then needs to be declared in distinct variable constraints. [[User:Kingdon|Kingdon]] 13:45, 25 July 2010 (UTC)